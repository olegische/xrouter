"""Deepseek API models.

This module contains Pydantic models for handling Deepseek provider requests and
responses, with specific handling for the Deepseek streaming format.
"""
import uuid
from datetime import datetime
from enum import Enum
from typing import Any, List, Literal, Optional

from pydantic import BaseModel, Field


class Role(str, Enum):
    """Message author roles."""

    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


class DeepseekObjectType(str, Enum):
    """Deepseek object types."""

    CHAT_COMPLETION = "chat.completion"
    CHAT_COMPLETION_CHUNK = "chat.completion.chunk"


class DeepseekStreamFunctionCall(BaseModel):
    """Function call model for streaming tool calls."""

    name: Optional[str] = Field(None, description="The name of the function to call")
    arguments: Optional[str] = Field(
        None, description="The arguments to call the function with"
    )


class DeepseekStreamToolCall(BaseModel):
    """Tool call model for streaming."""

    index: int = Field(description="The index of the tool call in the list")
    id: Optional[str] = Field(None, description="The ID of the tool call")
    type: Optional[Literal["function"]] = Field(
        None,
        description="The type of the tool. Currently, only `function` is supported.",
    )
    function: Optional[DeepseekStreamFunctionCall] = Field(
        None, description="The function that the model wants to call"
    )


class DeepseekDelta(BaseModel):
    """Deepseek delta message model for streaming.

    Note: role is only present in the first chunk, making it optional.
    """

    role: Optional[Role] = Field(
        None, description="Message author role: only present in first chunk"
    )
    content: Optional[str] = Field(None, description="Message content fragment")
    tool_calls: Optional[List[DeepseekStreamToolCall]] = Field(
        None, description="The tool calls generated by the model"
    )
    reasoning_content: Optional[str] = Field(
        None,
        description=(
            "For deepseek-reasoner model only. "
            "The reasoning contents before the final answer"
        ),
    )


class DeepseekStreamChoice(BaseModel):
    """Deepseek streaming choice model."""

    index: int = Field(0, description="Choice index in the array, starting from zero")
    delta: DeepseekDelta = Field(description="Partial updates to the message")
    logprobs: Optional[Any] = Field(None, description="Log probability information")
    finish_reason: Optional[
        Literal["stop", "length", "content_filter", "tool_calls", "function_call"]
    ] = Field(
        None,
        description=(
            "The reason the model stopped generating tokens: 'stop', 'length', "
            "'content_filter', 'tool_calls'"
        ),
    )


class CompletionTokensDetails(BaseModel):
    """Details about completion tokens."""

    reasoning_tokens: Optional[int] = Field(
        None, description="Tokens generated by the model for reasoning"
    )


class DeepseekUsage(BaseModel):
    """Deepseek usage statistics."""

    prompt_tokens: int = Field(
        description=(
            "Number of tokens in the prompt. "
            "Equals prompt_cache_hit_tokens + prompt_cache_miss_tokens"
        )
    )
    completion_tokens: int = Field(
        description="Number of tokens in the generated completion"
    )
    total_tokens: int = Field(
        description="Total number of tokens used in the request (prompt + completion)"
    )
    prompt_cache_hit_tokens: int = Field(
        description="Number of tokens in the prompt that hits the context cache"
    )
    prompt_cache_miss_tokens: int = Field(
        description="Number of tokens in the prompt that misses the context cache"
    )
    completion_tokens_details: Optional[CompletionTokensDetails] = Field(
        None, description="Breakdown of tokens used in a completion"
    )


class DeepseekStreamResponse(BaseModel):
    """Deepseek streaming response model."""

    id: str = Field(
        default_factory=lambda: str(uuid.uuid4()), description="Response identifier"
    )
    object: DeepseekObjectType = Field(
        default=DeepseekObjectType.CHAT_COMPLETION_CHUNK,
        description="Response type identifier",
    )
    created: int = Field(
        default_factory=lambda: int(datetime.utcnow().timestamp()),
        description="Response creation timestamp in Unix time",
    )
    model: str = Field(description="Model used for generation")
    system_fingerprint: Optional[str] = Field(
        None, description="System fingerprint for this model version"
    )
    choices: List[DeepseekStreamChoice] = Field(
        description="Array of streaming choices"
    )
    usage: Optional[DeepseekUsage] = Field(
        None, description="Token usage statistics for final chunks"
    )
